# Task_05_Descriptive_Stats
# Author - Bhaarat Kotak

## Objective

This repository is a submission for Research Task 05, which involves analyzing structured sports data using descriptive statistics and validating insights generated by a Large Language Model (LLM). The aim is to assess whether LLMs like ChatGPT can accurately answer natural language prompts using small, structured datasets.

The dataset used contains individual and team performance metrics from the 2024 season of SU Men’s Soccer.

## Datasets

Note: The dataset is not included in this repository to adhere to data privacy and size constraints. It must be uploaded manually.

## Datasets:
	•	su_mens_soccer_2024.csv

## Scripts

1. stats_summary_SU_mens_soccer_2024_BK.ipynb
	•	Loads player and team data using pandas and numpy.
	•	Computes advanced metrics:
	•	Goals + Assists per 90 minutes
	•	Shot percentage and shot-on-goal percentage
	•	Impact based on minutes played and games started
	•	Validates answers from ChatGPT for 10 natural language prompts using Python logic.
	•	Identifies discrepancies, hallucinations, or context failures in LLM responses.

## Instructions to Run

Make sure you have the required packages installed:

pip install pandas numpy

## Key Takeways

- **LLM Limitations**:
  - ChatGPT answered basic stat queries correctly but struggled with contextual logic (e.g., filtering players with enough minutes).
  - It failed to consider opponent data for performance comparisons in some prompts.

- **Python Validation**:
  - Ensures reliability by computing the same metrics programmatically.
  - Useful to benchmark LLM responses for accuracy and hallucination detection.

- **Recommendations**:
  - For high-stakes or analytical work, always pair LLMs with code-based validation.
  - Establish thresholds (e.g., minimum minutes or games played) to avoid skewed conclusions.
